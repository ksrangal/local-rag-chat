# local-rag-chat ðŸ¤–

## Description
The `local-rag-chat` project provides a local Retrieval-Augmented Generation (RAG) chat application. It allows users to ingest data from a specified directory, create embeddings using Ollama, store these embeddings in a Chroma vector database, and then use these embeddings to answer user queries via a Streamlit interface and a local language model. The application supports PDF and JSON file types for data ingestion.

## Features
- **Data Ingestion:** Ingests data from local PDF and JSON files.
- **Data Chunking:** Splits ingested documents into smaller chunks for better retrieval.
- **Embedding Generation:** Uses Ollama to generate embeddings for the document chunks.
- **Vector Storage:** Stores document embeddings in a Chroma vector database for efficient similarity search.
- **Querying:** Answers user queries by retrieving relevant document chunks from the vector database and using a language model.
- **Streamlit Interface:** Provides an interactive chat interface for users to interact with the RAG system.

## Tech Stack
- **Language:** Python
- **Frameworks:** Langchain, Streamlit
- **Vector Database:** Chroma
- **Embedding Model:** Ollama

## Installation
1.  Clone the repository:

    ```bash
    git clone https://github.com/ksrangal/local-rag-chat.git
    cd local-rag-chat
    ```

2.  Install the required dependencies:

    ```bash
    pip install -r requirements.txt
    ```

    The `requirements.txt` file includes the following:

    ```text
    streamlit
    langchain
    langchain-core
    langchain-chroma
    langchain-ollama
    chromadb
    ollama
    ```

## Usage
1.  Prepare your data: Place your `.pdf` and `.json` files in a directory.
2.  Run the Streamlit application:

    ```bash
    streamlit run local_rag_chat.py
    ```

3.  In the Streamlit interface:

    -   Enter the path to your data directory in the sidebar.
    -   Provide a prompt template in the sidebar.
    -   Ask your question in the chat input.
    -   The AI Assistant will respond with an answer based on the ingested data.



## How to Use
This project can be used to create a local, private chatbot that answers questions based on your own documents. Here are some real-world use cases:

-   **Personal Knowledge Base:** Create a chatbot that answers questions based on your notes, articles, and other documents.
-   **Internal Documentation Chatbot:** Allow employees to quickly find answers to questions about company policies, procedures, and other internal documentation.
-   **Research Assistant:** Use the chatbot to quickly find relevant information in research papers, articles, and other academic materials.

**Example Prompt Template:**

```text
I am a helpful assistant.

Here are some relevant documents: {reviews}

Here is the question to answer: {question}
```

## Project Structure
```
local-rag-chat/
â”œâ”€â”€ ingester.py         # Data ingestion and embedding logic
â”œâ”€â”€ local_rag_chat.py   # Main Streamlit application
â”œâ”€â”€ README.md           # Project documentation
â””â”€â”€ requirements.txt    # Project dependencies
```

## Contributing ðŸ™Œ
Contributions are welcome! Please feel free to submit pull requests or open issues to suggest improvements or report bugs.

## Important Links ðŸ”—
-   [Repository Link](https://github.com/ksrangal/local-rag-chat)
